{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we parse the connection details from the vcap.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.io.Source\n",
    "import play.api.libs.json._\n",
    "\n",
    "val json = Json.parse(Source.fromFile(\"vcap.json\").getLines().mkString)\n",
    "\n",
    "val bootstrap_servers = (json \\ \"kafka_brokers_sasl\").\n",
    "   toString().\n",
    "   replaceAll(\"\\\"\", \"\").\n",
    "   replaceAll(\"\\\\[\", \"\").\n",
    "   replaceAll(\"\\\\]\", \"\")\n",
    "val username = (json \\ \"user\").toString()\n",
    "val password = (json \\ \"password\").toString()\n",
    "val topic    = (json \\ \"topic\").toString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we connect to kafka.  When you run the next cell, it will run indefinitely  until you stop or interrupt the kernel.\n",
    "\n",
    "After running the cell, open the **Step 4** notebook in another tab to send some data to Message Hub and come back to the output under the cell below to see that the data is displayed by this consumer.  \n",
    "\n",
    "Note that it could take 60 seconds before the sent data is printed out by this consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import net.christophersnow.config.MessageHubConfig\n",
    "import net.christophersnow.dstream.KafkaStreaming.KafkaStreamingContextAdapter\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "\n",
    "import org.apache.kafka.common.serialization.Deserializer\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "\n",
    "val kafkaProps = new MessageHubConfig\n",
    "\n",
    "kafkaProps.setConfig(\"bootstrap.servers\",   bootstrap_servers)\n",
    "kafkaProps.setConfig(\"kafka.user.name\",     username)\n",
    "kafkaProps.setConfig(\"kafka.user.password\", password)\n",
    "kafkaProps.setConfig(\"kafka.topic\",         topic)\n",
    "\n",
    "kafkaProps.createConfiguration()\n",
    "\n",
    "val ssc = new StreamingContext( sc, Seconds(60) )\n",
    "\n",
    "val stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](\n",
    "  kafkaProps,\n",
    "  List(kafkaProps.getConfig(\"kafka.topic\"))\n",
    "  );\n",
    "\n",
    "stream.foreachRDD{ rdd =>\n",
    "  // we only want to create a folder in dsx if we have some data\n",
    "  if (rdd.count() > 0) {\n",
    "    def uuid = java.util.UUID.randomUUID.toString\n",
    "    val outDir = s\"test-${uuid}\"\n",
    "    rdd.saveAsTextFile (outDir)\n",
    "  }\n",
    "}\n",
    "\n",
    "stream.print()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
