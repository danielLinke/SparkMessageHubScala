{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bootstrap_servers = \"kafka05-prod01.messagehub.services.us-south.bluemix.net:9093,kafka03-prod01.messagehub.services.us-south.bluemix.net:9093,kafka04-prod01.messagehub.services.us-south.bluemix.net:9093,kafka01-prod01.messagehub.services.us-south.bluemix.net:9093,kafka02-prod01.messagehub.services.us-south.bluemix.net:9093\"\n",
    "val username = \"changeme\"\n",
    "val password = \"changeme\"\n",
    "val topic    = \"mytopic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n",
      "Registering JaasConfiguration: /gpfs/fs01/user/s15a-8ea34840daaa3e-39ca506ba762/notebook/tmp/bpyCprMeLfavhUmQ/jaas.conf\n",
      "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n",
      "-------------------------------------------\n",
      "Time: 1493061180000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1493061240000 ms\n",
      "-------------------------------------------\n",
      "(null,123)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import net.christophersnow.config.MessageHubConfig\n",
    "import net.christophersnow.dstream.KafkaStreaming.KafkaStreamingContextAdapter\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "\n",
    "import org.apache.kafka.common.serialization.Deserializer\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "\n",
    "val kafkaProps = new MessageHubConfig\n",
    "\n",
    "kafkaProps.setConfig(\"bootstrap.servers\",   bootstrap_servers)\n",
    "kafkaProps.setConfig(\"kafka.user.name\",     username)\n",
    "kafkaProps.setConfig(\"kafka.user.password\", password)\n",
    "kafkaProps.setConfig(\"kafka.topic\",         topic)\n",
    "\n",
    "kafkaProps.createConfiguration()\n",
    "\n",
    "val ssc = new StreamingContext( sc, Seconds(60) )\n",
    "\n",
    "val stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](\n",
    "  kafkaProps,\n",
    "  List(kafkaProps.getConfig(\"kafka.topic\"))\n",
    "  );\n",
    "\n",
    "stream.foreachRDD{ rdd =>\n",
    "  // we only want to create a folder in hdfs if we have some data\n",
    "  if (rdd.count() > 0) {\n",
    "    def uuid = java.util.UUID.randomUUID.toString\n",
    "    val outDir = s\"test-${uuid}\"\n",
    "    rdd.saveAsTextFile (outDir)\n",
    "  }\n",
    "}\n",
    "\n",
    "stream.print()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}